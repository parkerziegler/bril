# Simple and Efficient Construction of Static Single Assignment Form

_Matthias Braun, Sebastian Buchwald, Sebastian Hack, Roland Leißa, Christoph Mallon, and Andreas Zwinkau. CC'13: Proceedings of the 22nd International Conference on Compiler Construction, Mar 2013, Pages 102-122._

## Commentary

This paper exemplifies a genre of research I've increasingly come to appreciate—taking a "solved" problem, questioning previous solutions' core framing and tactics, and arriving at an (arguably) better alternative _via simplification_. The real payoff here, to me, is the ability of Braun et al.'s algorithm to avoid (1) the placement of dead Φ functions and (2) the need for a "detour" into a non-SSA control flow graph to compute dominance frontiers and perform liveness analysis and dead code elimination. This insight pushed me to think more about the missed opportunity costs of isolated analyses that compute one narrow, well-defined set of facts about a program. In the worst cases, too strong of a commitment to isolation pushes us towards performing significantly _more_ work by failing to take advantage of the broader informational context. This is something I've been thinking about in my own research on incremental evaluation and self-adjusting computation in the context of direct manipulation programming interfaces.

While the main takeway felt plenty satisfying, this paper also stood out to me for its handful of clever algorithmic tactics. I think the most fascinating for me were _trivial_ Φ function elimination and the use of operands as "witnesses" in a memoization strategy. To the first point, I was particularly struck by how Braun et al. manage to turn placement and removal of Φ functions into a _simultaneous_ process. This seems to stand in stark contrast to Cytron et al.'s algorithm which—from my understanding in class—operates as a two-phase sequential process of Φ function addition and elimination. The recursion in Algorithm 3 seemed to be the underlying workhorse here, allowing propagation of newly uncovered trivial Φ functions to its uses and, in turn, potentially trivializing _those_ Φ functions. Just beautiful!

To the second point on "witnesses", this appears to be the dual side of making this recursion efficient. It's a clever observation that determining a Φ function's triviality _only_ requires inspection of its first two operands; even then, if one operand has been eliminated, we need only search for one more unique witness before re-checking triviality. I love techniques like this that use a subset of all information about a data structure as a proxy for some larger property we want to check. In many ways, this memoization / tabling technique reminds me of some recent reading I've been doing on data structures called "signals", which are increasingly used in UI frameworks embracing functional reactive programming paradigms. Signals are based on a dependency graph between time-varying values and share a similar notion of being able to "break" or "mark off" subgraphs as not needing recomputation based on a subset of information about incoming edges to the subgraph. Really neat stuff!

## Lingering Questions

I was left with one lingering question—primarily conceptual—after reading this paper. The paper discusses the notion of _sealed_ basic blocks, blocks for which no predecessors will be added. It also discusses the strategy of placing operandless Φ functions into _unsealed_ basic blocks, which are given operands at a later time when the block gets sealed. To me, this seemed to be similar to Cytron's algorithm, where our first phase inserted operandless Φ functions (or rather, Φ functions with placeholder operands) and our second phase concretized the operands as part of the renaming process. So to my question—is there something unique or different going on here between Cytron and Braun? Is it just that sealing is an action taken at IR construction time, so this replacement of operandless Φ functions with "operand-ful" Φ functions is happening _earlier_ / not in a second pass?
