# Lazy Code Motion

_Jens Knoop, Oliver Rüthing, and Bernhard Steffen. Proceedings of the ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation, July 1992, Pages 224-234._

## Commentary

Truthfully, this paper was a bit of a slog for me. The avalanche of lemmas in addition to the wave of new definitions ("down-safe", "ds-earliestness", "isolated", etc.) was exceedingly difficult to parse, largely due to the cognitive load of keeping all the formalism straight. But in combination with the linked slides from CMU's 15-745, I think I have a loose grasp on the core idea.

The standout insight in this paper to me was the phased approach of the lazy code motion algorithm, where each successive analysis appears to derive some new information to subsequently improve the placement of moved code. Again, it speaks to this recurrent theme that _combining_ analyses can yield more powerful results than applying them in isolation. My understanding is that the first two analyses—D-Safe (forward) and Earliest (backwards)—work to identify the _computationally optimal_ placement, i.e., the placement that requires the minimal number of computations for a given term `t` at runtime. It took me a little to see that _computationally optimal_, in the authors' definition, may still result in unnecessary code motion; hence the need for the subsequent two passes, Delay and Latest. The strategy almost seems to be to use the first two passes as a safe overapproximation (find the nodes in the CFG for which the Safe-Earliest Transformation holds) and the latter two phases as an iterative optimizer that (1) moves code as late as possible while (2) preserving the safety properties. In this sense, I see the role of the first two passes as establishing the outer boundaries of possible placements, while the latter two phases work to refine these boundaries inwards.

Zooming out, I can see a lot of benefits to this strategy as a general algorithmic approach not only for static analyses but also for related compiler engineering tasks I'm working on currently, like code generation. In my current research system, I often have choices to make around placement of generated code that involves code motion (e.g., hoisting functions that are used in multiple local scopes into the "least outer scope"), and to this point I've taken a very conservative approach that misses out on opportunities for partial redundancy elimination. So, if there's any takeaway for me from this paper, it's to investigate this tactic of combining analyses to first establish safety and then iteratively move code towards more optimal placements on-demand while ensuring preservation.

Finally, I'll end with a light critique which is—where was the evaluation?! The authors make the contention that the efficiency result of their algorithm over prior bidirectional algorithms (O (n^2)) is implied by their use of unidirectional analyses (O n(log n)). But they have four of these analyses running! I'd be very curious to see in practice—on practical programs—how the comparison might shake out.
